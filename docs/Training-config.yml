
# Parameters for microDL virtual staining model training

# The following configuration file defines the parameters to go from data processing to optimizing the microDL model.
# Inorder to optimize the microDL model, which includes building the network and optimizing the weights, the input data provided is utilized.
# Tiles of input data with different forms of noise and aberrations will accustom the model to work with future data which will contain basically similar
# biological information but will be perturbed by different conditions (like change in image quality with differing imaging systems,
# difference in sample condition, due to difference in cells, infections, drug treatments, cell, confluence, etc).
# Initial conditions for initiating learning by the model is also provided, such as learning rate, step size, number of epochs, etc.

# dataset parameters defining the different channels, how to split the dataset for training, validation and testing,
# and image processing used for diversifying the data conditions, used for training the model and thus optimizing the weights.
dataset:

# Specify the directory in which your preprocessed tiles are stored
  data_dir: '/home/Processed_temp_dir/tiles_256-256_step_128-128'

# specify the model task, if it is regression model
  model_task: regression

# Defines the tile height
  height: 256

# specify the label-free channel to be used for predicting the stained image
  input_channels: [2]

# This is the extra channel which has been added to store the mask produced by segmenting the fluorescence stained image channel.
  mask_channels: [3]    # This will be last input channel id + 1

# define if preprocessing is performed?
  preprocess: False

# Define on what basis you want to split the input data folder for creating training image set, validation set and test set
  split_by_column: pos_idx  # here 'pos_idx' indicates the data set was divided based on position list

# Define the split ratio of data set
  split_ratio:

# here test:0.1 , train : 0.8, val:0.1, indicates 80% of images from the position list were used for training the model,
# 10% were used for validation and 10% for testing
    test: 0.1
    train: 0.8
    val: 0.1
  squeeze: True
  target_channels:
  - 1
  width: 256
  random_seed: 0

# there parameters defines the scales used for different augmentations used on input tiles for training the model
  augmentations:
    zoom_range: [0.6, 1.6]           # different magnification ranges the data can be in
    rotate_range: 180                # 180 indicates all rotation degrees from 0 to 180 degrees
    intensity_jitter: [0.5, 0.5]     # Jitter in image intensity to accomodate the varying intensities seen in images
    noise_std: 0.5                   # Adds noise to images with standard deviation of 0.5
    blur_range: [0, 10]              # Blurs the image from nil to range 10, accommodating conditions like images blurring at low magnification
    shear_range: 10                  # Shear images to range of 10, accommodates cells stretching and deformation
    

# The upcoming parameters defines the microDL model used for training and required to set off training
network:

# Activation type shows activation layer type in the learning model, which helps process weighted inputs to deliver the output
  activation:
    type: relu    # relu is rectified linear unit, the activation function

# specify if the input is batch normalized
  batch_norm: True
  block_sequence: conv-activation-bn

# Specify if the model class is UNet2D or UNet2.5D or UNet3D. 3DUNet is heavy to run compared to 2.5D, thus making 2.5D more
# efficient for use with 3D input data. 2DUNet is useful if using 2D data
  class: UNet2D

#
  data_format: channels_first
  dropout: 0.2
  dropout_dense: 0.4
  filter_size: 3
  final_activation: linear
  temperature: 1
  height: 256
  num_convs_per_block: 2

# defines the number of filters used in each layer of the model, optimizing the weights as training progresses
  num_filters_per_block:
  - 16
  - 32
  - 64
  - 128
  - 256

# Number of input channels, can be 1 if only phase images are input, can be 2 if both phase and retardance are used
  num_input_channels: 1

# Number of target channels defines the number of fluorescence channels to be predicted.
  num_target_channels: 1     # 1 as only nuclear channels is to be predicted

#
  pooling_type: average
  residual: True
  skip_merge_type: concat
  upsampling: bilinear
  width: 256

# Specify the parameters to define the sensitivity and speed with which to train the model.
# You want to make sure the model trains enough to minimize the loss function. This improves the prediction.
# But you also want to avoid overfitting. This is taken care of by terminating the training when the training loss is low
# and the validation loss gets low, but is terminated as soon as it starts to increase. If the validation loss becomes too
# low compared to training loss it indicates the model is overfitting.

trainer:  
  batch_size: 64   # number depends on compute resource available. Reduce or increase to accommodate to your system.
  callbacks:
    LearningRateScheduler:
      lr_find: False         # model iteratively finds the learning rate by itself
      base_lr: 0.00005       # define the base learning rate
      max_lr: 0.006          # specify the maximum learning rate
      step_size: 2           # step size
      gamma: 0.5
      scale_mode: "cycle"
    EarlyStopping:
      mode: min              # stop the model if validation loss is low
      monitor: val_loss
      patience: 10
      verbose: True
    ModelCheckpoint:
      mode: min              # max - val loss does not increase anymore
      monitor: val_loss      # val_dice_coef
      save_best_only: True
      verbose: True
    TensorBoard:
      histogram_freq: 0
      verbose: True
  loss: mae_loss             # specify if mean absolute error (mae) or mean squared error (mse), mae preferred
  masked_loss: false
  max_epochs: 200            # maximum number of epochs, should be more than number of epochs required to get to low loss value
  metrics: pearson_corr
  model_dir: '/home/Translation_temp_dir'   # save the trained model in the following directory
  optimizer:
    lr: 0.0001
    name: adam
  patience: 10
  num_workers: 32      # depends on the number of threads on GPU used for training, increase if more threads available
verbose: 10