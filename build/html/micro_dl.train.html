<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>micro_dl.train module &mdash; microDL 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="micro_dl.utils module" href="micro_dl.utils.html" />
    <link rel="prev" title="micro_dl.preprocessing module" href="micro_dl.preprocessing.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            microDL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">micro_dl</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="micro_dl.html">micro_dl module</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="micro_dl.html#submodules">Submodules</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="micro_dl.cli.html">micro_dl.cli module</a></li>
<li class="toctree-l4"><a class="reference internal" href="micro_dl.inference.html">micro_dl.inference module</a></li>
<li class="toctree-l4"><a class="reference internal" href="micro_dl.input.html">micro_dl.input module</a></li>
<li class="toctree-l4"><a class="reference internal" href="micro_dl.networks.html">micro_dl.networks module</a></li>
<li class="toctree-l4"><a class="reference internal" href="micro_dl.plotting.html">micro_dl.plotting module</a></li>
<li class="toctree-l4"><a class="reference internal" href="micro_dl.preprocessing.html">micro_dl.preprocessing module</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">micro_dl.train module</a></li>
<li class="toctree-l4"><a class="reference internal" href="micro_dl.utils.html">micro_dl.utils module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="micro_dl.html#module-micro_dl">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">microDL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">micro_dl</a></li>
          <li class="breadcrumb-item"><a href="micro_dl.html">micro_dl module</a></li>
      <li class="breadcrumb-item active">micro_dl.train module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/micro_dl.train.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="micro-dl-train-module">
<h1>micro_dl.train module<a class="headerlink" href="#micro-dl-train-module" title="Permalink to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading"></a></h2>
</section>
<section id="module-micro_dl.train.learning_rates">
<span id="micro-dl-train-learning-rates-module"></span><h2>micro_dl.train.learning_rates module<a class="headerlink" href="#module-micro_dl.train.learning_rates" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="micro_dl.train.learning_rates.CyclicLearning">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">micro_dl.train.learning_rates.</span></span><span class="sig-name descname"><span class="pre">CyclicLearning</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.006</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cycle'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.learning_rates.CyclicLearning" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Custom Callback implementing cyclical learning rate (CLR) as in the paper:
<a class="reference external" href="https://arxiv.org/abs/1506.01186">https://arxiv.org/abs/1506.01186</a>.</p>
<p>Learning rate is increased then decreased in a repeated triangular
pattern over time. One triangle = one cycle.
step-size is the number of iterations / batches in half a cycle.
The paper recommends a step-size of 2-10 times the number of batches in
an epoch (empirical) i.e step-size = 2-10 epochs.
Also best to stop training at the end of a cycle when the learning rate is
at minimum value and accuracy/performance potentially peaks.
Initial amplitude is scaled by gamma ** iterations.
<a class="reference external" href="https://keras.io/callbacks/">https://keras.io/callbacks/</a>
<a class="reference external" href="https://github.com/bckenstler/CLR">https://github.com/bckenstler/CLR</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="micro_dl.train.learning_rates.CyclicLearning.clr">
<span class="sig-name descname"><span class="pre">clr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.learning_rates.CyclicLearning.clr" title="Permalink to this definition"></a></dt>
<dd><p>Updates the cyclic learning rate with exponential decline.</p>
<dl class="field-list simple">
<dt class="field-odd">Return float clr<span class="colon">:</span></dt>
<dd class="field-odd"><p>Learning rate as a function of iterations</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="micro_dl.train.learning_rates.CyclicLearning.on_batch_end">
<span class="sig-name descname"><span class="pre">on_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.learning_rates.CyclicLearning.on_batch_end" title="Permalink to this definition"></a></dt>
<dd><p>Updates the learning rate at the end of each batch. Prints
learning rate along with other metrics during training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – Batch number from Callback super class</p></li>
<li><p><strong>logs</strong> – Log from super class (required but not used here)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="micro_dl.train.learning_rates.CyclicLearning.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.learning_rates.CyclicLearning.on_epoch_end" title="Permalink to this definition"></a></dt>
<dd><p>Log learning rate at the end of each epoch for
Tensorboard and CSVLogger.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – Epoch number from Callback super class</p></li>
<li><p><strong>logs</strong> – Log from super class</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="micro_dl.train.learning_rates.CyclicLearning.on_train_begin">
<span class="sig-name descname"><span class="pre">on_train_begin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.learning_rates.CyclicLearning.on_train_begin" title="Permalink to this definition"></a></dt>
<dd><p>Set base learning rate at the beginning of training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>logs</strong> – Logging from super class</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-micro_dl.train.losses">
<span id="micro-dl-train-losses-module"></span><h2>micro_dl.train.losses module<a class="headerlink" href="#module-micro_dl.train.losses" title="Permalink to this heading"></a></h2>
<p>Custom losses</p>
<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.binary_crossentropy_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">binary_crossentropy_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.binary_crossentropy_loss" title="Permalink to this definition"></a></dt>
<dd><p>Binary cross entropy loss
:param y_true: Ground truth
:param y_pred: Prediction
:return float: Binary cross entropy loss</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.dice_coef_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">dice_coef_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.dice_coef_loss" title="Permalink to this definition"></a></dt>
<dd><p>The Dice loss function is defined by 1 - DSC
since the DSC is in the range [0,1] where 1 is perfect overlap
and we’re looking to minimize the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – true values</p></li>
<li><p><strong>y_pred</strong> – predicted values</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Dice loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.dssim_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">dssim_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.dssim_loss" title="Permalink to this definition"></a></dt>
<dd><p>Structural dissimilarity loss + L1 loss
DSSIM is defined as (1-SSIM)/2
<a class="reference external" href="https://en.wikipedia.org/wiki/Structural_similarity">https://en.wikipedia.org/wiki/Structural_similarity</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>tensor</em>) – Labeled ground truth</p></li>
<li><p><strong>y_pred</strong> (<em>tensor</em>) – Predicted labels, potentially non-binary</p></li>
</ul>
</dd>
<dt class="field-even">Return float<span class="colon">:</span></dt>
<dd class="field-even"><p>0.8 * DSSIM + 0.2 * L1</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.kl_divergence_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">kl_divergence_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.kl_divergence_loss" title="Permalink to this definition"></a></dt>
<dd><p>KL divergence loss
D(y||y’) = sum(p(y)*log(p(y)/p(y’))</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – Ground truth</p></li>
<li><p><strong>y_pred</strong> – Prediction</p></li>
</ul>
</dd>
<dt class="field-even">Return float<span class="colon">:</span></dt>
<dd class="field-even"><p>KL divergence loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.latent_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">latent_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dummy_ground_truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.latent_loss" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.mae_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">mae_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.mae_loss" title="Permalink to this definition"></a></dt>
<dd><p>Mean absolute error</p>
<p>Keras losses by default calculate metrics along axis=-1, which works with
image_format=’channels_last’. The arrays do not seem to batch flattened,
change axis if using ‘channels_first</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.masked_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">masked_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_channels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.masked_loss" title="Permalink to this definition"></a></dt>
<dd><p>Converts a loss function to mask weighted loss function</p>
<p>Loss is multiplied by mask. Mask could be binary, discrete or float.
Provides different weighting of loss according to the mask.
<a class="reference external" href="https://github.com/keras-team/keras/blob/master/keras/engine/training_utils.py">https://github.com/keras-team/keras/blob/master/keras/engine/training_utils.py</a>
<a class="reference external" href="https://github.com/keras-team/keras/issues/3270">https://github.com/keras-team/keras/issues/3270</a>
<a class="reference external" href="https://stackoverflow.com/questions/46858016/keras-custom-loss-function-to-pass-arguments-other-than-y-true-and-y-pred">https://stackoverflow.com/questions/46858016/keras-custom-loss-function-to-pass-arguments-other-than-y-true-and-y-pred</a></p>
<p>nested functions -&gt; closures
A Closure is a function object that remembers values in enclosing
scopes even if they are not present in memory. Read only access!!
Histogram and logical operators are not differentiable, avoid them in loss
modified_loss = tf.Print(modified_loss, [modified_loss],</p>
<blockquote>
<div><p>message=’modified_loss’, summarize=16)</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_fn</strong> (<em>Function</em>) – a loss function that returns a loss image to be
multiplied by mask</p></li>
<li><p><strong>n_channels</strong> (<em>int</em>) – number of channels in y_true. The mask is added as
the last channel in y_true</p></li>
</ul>
</dd>
</dl>
<p>:return function masked_loss_fn</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.ms_ssim_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">ms_ssim_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.ms_ssim_loss" title="Permalink to this definition"></a></dt>
<dd><p>Multiscale structural dissimilarity loss + L1 loss
Uses the same combination weight as the original paper by Wang et al.:
<a class="reference external" href="https://live.ece.utexas.edu/publications/2003/zw_asil2003_msssim.pdf">https://live.ece.utexas.edu/publications/2003/zw_asil2003_msssim.pdf</a>
Tensorflow doesn’t have a 3D version so for stacks the MS-SSIM is the
mean of individual slices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>tensor</em>) – Labeled ground truth</p></li>
<li><p><strong>y_pred</strong> (<em>tensor</em>) – Predicted labels, potentially non-binary</p></li>
</ul>
</dd>
<dt class="field-even">Return float<span class="colon">:</span></dt>
<dd class="field-even"><p>ms-ssim loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.losses.mse_loss">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.losses.</span></span><span class="sig-name descname"><span class="pre">mse_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.losses.mse_loss" title="Permalink to this definition"></a></dt>
<dd><p>Mean squared loss</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – Ground truth</p></li>
<li><p><strong>y_pred</strong> – Prediction</p></li>
</ul>
</dd>
<dt class="field-even">Return float<span class="colon">:</span></dt>
<dd class="field-even"><p>Mean squared error loss</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-micro_dl.train.lr_finder">
<span id="micro-dl-train-lr-finder-module"></span><h2>micro_dl.train.lr_finder module<a class="headerlink" href="#module-micro_dl.train.lr_finder" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="micro_dl.train.lr_finder.LRFinder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">micro_dl.train.lr_finder.</span></span><span class="sig-name descname"><span class="pre">LRFinder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fig_fname</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.lr_finder.LRFinder" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="micro_dl.train.lr_finder.LRFinder.on_batch_end">
<span class="sig-name descname"><span class="pre">on_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.lr_finder.LRFinder.on_batch_end" title="Permalink to this definition"></a></dt>
<dd><p>Increase learning rate gradually after each batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – Batch number from Callback super class</p></li>
<li><p><strong>logs</strong> – Log from super class (required)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="micro_dl.train.lr_finder.LRFinder.on_train_begin">
<span class="sig-name descname"><span class="pre">on_train_begin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.lr_finder.LRFinder.on_train_begin" title="Permalink to this definition"></a></dt>
<dd><p>Set base learning rate at the beginning of training and get step size
for learning rate increase</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>logs</strong> – Logging from super class</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="micro_dl.train.lr_finder.LRFinder.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.lr_finder.LRFinder.on_train_end" title="Permalink to this definition"></a></dt>
<dd><p>After finishing the increase from base_lr to max_lr, save plot.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>logs</strong> – Log from super class</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-micro_dl.train.metrics">
<span id="micro-dl-train-metrics-module"></span><h2>micro_dl.train.metrics module<a class="headerlink" href="#module-micro_dl.train.metrics" title="Permalink to this heading"></a></h2>
<p>Custom metrics</p>
<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.binary_accuracy">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">binary_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.binary_accuracy" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the mean accuracy rate across all predictions for binary
classification problems.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.coeff_determination">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">coeff_determination</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.coeff_determination" title="Permalink to this definition"></a></dt>
<dd><p>R^2 Goodness of fit, using as a proxy for accuracy in regression</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – Ground truth</p></li>
<li><p><strong>y_pred</strong> – Prediction</p></li>
</ul>
</dd>
<dt class="field-even">Return float r2<span class="colon">:</span></dt>
<dd class="field-even"><p>Coefficient of determination</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.dice_coef">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">dice_coef</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.dice_coef" title="Permalink to this definition"></a></dt>
<dd><p>This is a global non-binary Dice similarity coefficient (DSC)
with smoothing.
It computes an approximation of Dice but over the whole batch,
and it leaves predicted output as continuous. This might help
alleviate potential discontinuities a binary image level Dice
might introduce.
DSC = 2 * <a href="#id7"><span class="problematic" id="id8">|A union B|</span></a> /(<a href="#id9"><span class="problematic" id="id10">|A|</span></a> + <a href="#id11"><span class="problematic" id="id12">|B|</span></a>) = 2 * <a href="#id13"><span class="problematic" id="id14">|ab|</span></a> / (<a href="#id15"><span class="problematic" id="id16">|a|^2 + |b|^2)
where a, b are binary vectors
smoothed DSC = (2 * |ab|</span></a> + s) / (<a href="#id1"><span class="problematic" id="id2">|</span></a>a|^2 + <a href="#id3"><span class="problematic" id="id4">|</span></a>b|^2 + s)
where s is smoothing constant.
Although y_pred is not binary, it is assumed to be near binary
(sigmoid transformed) so <a href="#id5"><span class="problematic" id="id6">|</span></a>y_pred|^2 is approximated by sum(y_pred).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>tensor</em>) – Labeled ground truth</p></li>
<li><p><strong>y_pred</strong> (<em>tensor</em>) – Predicted labels, potentially non-binary</p></li>
<li><p><strong>smooth</strong> (<em>float</em>) – Constant added for smoothing and to avoid
divide by zeros</p></li>
</ul>
</dd>
<dt class="field-even">Return float dice<span class="colon">:</span></dt>
<dd class="field-even"><p>Smoothed non-binary Dice coefficient</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.flip_dimensions">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">flip_dimensions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">func</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.flip_dimensions" title="Permalink to this definition"></a></dt>
<dd><p>Decorator to convert channels first tensor to channels last.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>func</strong> – Function to be decorated</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.mask_accuracy">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">mask_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_channels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.mask_accuracy" title="Permalink to this definition"></a></dt>
<dd><p>split y_true into y_true and mask</p>
<p>For masked_loss there’s an added function/method to split
y_true and pass to loss, metrics and callbacks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>n_channels</strong> (<em>int</em>) – Number of channels</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.mask_coeff_determination">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">mask_coeff_determination</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_channels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.mask_coeff_determination" title="Permalink to this definition"></a></dt>
<dd><p>split y_true into y_true and mask</p>
<p>For masked_loss there’s an added function/method to split
y_true and pass to loss, metrics and callbacks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>n_channels</strong> (<em>int</em>) – Number of channels</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.ms_ssim">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">ms_ssim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.ms_ssim" title="Permalink to this definition"></a></dt>
<dd><p>Shifts dimensions to channels last if applicable, before
calling func.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>tensor</em>) – Gound truth data</p></li>
<li><p><strong>y_pred</strong> (<em>tensor</em>) – Predicted data</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>function called with channels last</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.pearson_corr">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">pearson_corr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.pearson_corr" title="Permalink to this definition"></a></dt>
<dd><p>Pearson correlation
:param tensor y_true: Labeled ground truth
:param tensor y_pred: Predicted label,</p>
<dl class="field-list simple">
<dt class="field-odd">Return float r<span class="colon">:</span></dt>
<dd class="field-odd"><p>Pearson over all images in the batch</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="micro_dl.train.metrics.ssim">
<span class="sig-prename descclassname"><span class="pre">micro_dl.train.metrics.</span></span><span class="sig-name descname"><span class="pre">ssim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.metrics.ssim" title="Permalink to this definition"></a></dt>
<dd><p>Shifts dimensions to channels last if applicable, before
calling func.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>tensor</em>) – Gound truth data</p></li>
<li><p><strong>y_pred</strong> (<em>tensor</em>) – Predicted data</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>function called with channels last</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-micro_dl.train.trainer">
<span id="micro-dl-train-trainer-module"></span><h2>micro_dl.train.trainer module<a class="headerlink" href="#module-micro_dl.train.trainer" title="Permalink to this heading"></a></h2>
<p>Keras trainer</p>
<dl class="py class">
<dt class="sig sig-object py" id="micro_dl.train.trainer.BaseKerasTrainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">micro_dl.train.trainer.</span></span><span class="sig-name descname"><span class="pre">BaseKerasTrainer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sess</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_target_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_mem_frac</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.95</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.trainer.BaseKerasTrainer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Keras training class</p>
<dl class="py method">
<dt class="sig sig-object py" id="micro_dl.train.trainer.BaseKerasTrainer.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#micro_dl.train.trainer.BaseKerasTrainer.train" title="Permalink to this definition"></a></dt>
<dd><p>Train the model</p>
<p><a class="reference external" href="https://stackoverflow.com/questions/44747288/keras-sample-weight-array-error">https://stackoverflow.com/questions/44747288/keras-sample-weight-array-error</a>
<a class="reference external" href="https://gist.github.com/andreimouraviev/2642384705034da92d6954dd9993fb4d">https://gist.github.com/andreimouraviev/2642384705034da92d6954dd9993fb4d</a>
<a class="reference external" href="https://github.com/keras-team/keras/issues/2115">https://github.com/keras-team/keras/issues/2115</a></p>
<p>Suggested: modify generator to return a tuple with (input, output,
sample_weights) and use sample_weight_mode=temporal. This doesn’t fit
the case for dynamic weighting (i.e. weights change with input image)
Use model.fit instead of fit_generator as couldn’t find how sample
weights are passed from generator to fit_generator / fit.</p>
<p>FOUND A HACKY WAY TO PASS DYNAMIC WEIGHTS TO LOSS FUNCTION IN KERAS!
<a class="reference external" href="https://groups.google.com/forum/#!searchin/keras-users">https://groups.google.com/forum/#!searchin/keras-users</a>/pass$20custom$20loss$20|sort:date/keras-users/ue1S8uAPDKU/x2ml5J7YBwAJ</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-micro_dl.train">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-micro_dl.train" title="Permalink to this heading"></a></h2>
<p>Module for train functions</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="micro_dl.preprocessing.html" class="btn btn-neutral float-left" title="micro_dl.preprocessing module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="micro_dl.utils.html" class="btn btn-neutral float-right" title="micro_dl.utils module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, mehta-lab.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>